{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Outside the Warehouse with DuckDB + Python\n",
    "* PyData Global 2024 demo\n",
    "* DuckDb blog (great resources): https://duckdb.org/news/\n",
    "* coiled.io: https://docs.coiled.io/user_guide/index.html\n",
    "* github link : **https://github.com/AdarshNamala/PyDataGlobal_2024**\n",
    "* slides: https://docs.google.com/presentation/d/1q-i1sU_WaL-Fzm6dYwwS5e57DUOwV6YTpTvTvn77_hM/edit#slide=id.p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb \n",
    "import os\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import textwrap\n",
    "from datetime import datetime,UTC\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "from functools import partial\n",
    "import re\n",
    "import s3fs\n",
    "from time import sleep\n",
    "\n",
    "import coiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"version\")\n",
    "print(\"duckdb: \",duckdb.__version__)\n",
    "print(\"pandas: \",pd.__version__)\n",
    "print(\"python: \",sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_used():\n",
    "    \"\"\" Get current memory used \"\"\"\n",
    "    _mbs = psutil.Process().memory_full_info().uss/(1024**2)\n",
    "    print(f'Memory used: {_mbs:,.0f} MBs') \n",
    "\n",
    "    \n",
    "def get_months(start: str,end :str) -> list:\n",
    "    \"\"\" Function to get the months between the start and end date \"\"\"\n",
    "    return pd.date_range(start,end,freq='MS').strftime('%Y-%m')\n",
    "\n",
    "def get_s3_files_info(files):\n",
    "    \"\"\" get the file metadata from s3 using s3fs\"\"\"\n",
    "    return pd.DataFrame([_s3fs.info(_file) for _file in files])\n",
    "\n",
    "# other functions\n",
    "%run utility.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_used()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDB \n",
    "* in-memory and OLAP\n",
    "* Natively Read PyArrow, Pandas, R, Polars, etc., \n",
    "* DuckDB WASM!\n",
    "* Can read\n",
    "    * Parquet, CSV, json, iceberg, delta (expr), etc., \n",
    "    * from: Local, https, AWS, GCP, Azure, etc., \n",
    "* Connection: \n",
    "    * There are various ways to create the connection object\n",
    "        * in-memory only (RAM only). Does not support out of core operations \n",
    "        * in-memory with temporary storage:  allows out of core operation. When the connection is closed the temp storage is not persisted\n",
    "        * persisted database:  in-memory + out of core operation + persistent db that can be re-used. \n",
    "\n",
    "* YT videos on DuckDB:\n",
    "    * https://www.youtube.com/@duckdb/videos (DuckDB official channel)\n",
    "    * https://www.youtube.com/@motherduckdb/playlists (MotherDuck channel)\n",
    "    * https://www.youtube.com/watch?v=fZj6kTwXN1U&list=PLw2SS5iImhEThtiGNPiNenOr2tVvLj6H7 (Learn Data with Mark)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_persistent_db = \"~/Desktop/PyDataGlobal2024/storage.ddb\"\n",
    "con = duckdb.connect(_persistent_db) \n",
    "\n",
    "# other options to create connection: \n",
    "# duckdb.connection(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"~/.aws/credentials\" file is setup, duckdb can use the credentials to setup AWS keys\n",
    "_qry = f\"\"\"\n",
    "INSTALL AWS; LOAD AWS;\n",
    "CREATE or replace SECRET secret2 (TYPE S3, PROVIDER CREDENTIAL_CHAIN);\n",
    "\"\"\"\n",
    "con.sql(_qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_db_size():\n",
    "    \"\"\" \"\"\"\n",
    "    display(con.sql(\"call pragma_database_size()\").df())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "* We will use the popular NYC Taxi Data: \n",
    "* read parquet files via https"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nytaxi_data_path(month:str|list) -> str:\n",
    "    \"\"\" \n",
    "    Funciton to get the month path for NYC taxi\n",
    "    https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "    params\n",
    "        month(str): yyyy-mm format\n",
    "    \"\"\"\n",
    "    # You can get the base path by copying the link address from the website    \n",
    "    _base_path = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata\"\n",
    "\n",
    "    return f\"{_base_path}_{month}.parquet\"\n",
    "\n",
    "# get 2024 months. Get Jan - Sep 2024 files\n",
    "months = get_months(\"2024-01\",\"2024-09\")\n",
    "nytaxi_files = [get_nytaxi_data_path(month) for month in months]\n",
    "nytaxi_files[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modular SQL pipeline with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a fucntion that you can reuse\n",
    "def ddb_read_parquet(files:str|list[str],\n",
    "                     columns:list[str]=[]) -> str:\n",
    "    \"\"\" \n",
    "    Function to read parquet file into duckdb\n",
    "    params:\n",
    "        * files(str | list): file path of list of file_path\n",
    "        * cus\n",
    "    \"\"\"\n",
    "    _cols = ',\\n'.join(columns) or \"*\"\n",
    "    _files = files if isinstance(files,list) else [files]\n",
    "    # placeholder to add more params\n",
    "    _read_params = \"filename=true\"\n",
    "    \n",
    "    _qry = f\"\"\" \n",
    "    select \n",
    "        {_cols}\n",
    "    from read_parquet({_files},{_read_params})\n",
    "    \"\"\"\n",
    "    \n",
    "    return _qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_read_qry = ddb_read_parquet(nytaxi_files[:2])\n",
    "print(_read_qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute the query\n",
    "_read_qry = ddb_read_parquet(nytaxi_files[:1])\n",
    "con.sql(_read_qry).limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1==0:\n",
    "    _qry = ddb_read_parquet(files=nytaxi_files)\n",
    "    _cr_qry = f\"\"\" \n",
    "    create or replace table ny_taxi as {_qry}\n",
    "    \"\"\"\n",
    "    con.sql(_cr_qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\"show tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_db_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql('select * from ny_taxi').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aggregate: daily, monthly, etc.,**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_fare_qry(input_tb: str, index_col:str) -> str:\n",
    "    \"\"\" \n",
    "    helper function to get aggregate query for avg fare\n",
    "    \"\"\" \n",
    "    _agg_qry = f\"\"\" \n",
    "        select \n",
    "            {index_col},\n",
    "            total_amount.sum().round()::int as tot_amt,\n",
    "            sum(trip_distance).round()::int as tot_dist,\n",
    "            (tot_amt/tot_dist).round(2) as avg_fare,\n",
    "            count(*) as counts\n",
    "        from {input_tb}\n",
    "        group by all\n",
    "    \"\"\"\n",
    "    return _agg_qry\n",
    "\n",
    "\n",
    "def get_daily_avg_fare_qry(input_tb: str) -> str:\n",
    "    \"\"\" Function to get daily avg fare\"\"\"\n",
    "    index = \"tpep_pickup_datetime::date as pickup_date\"\n",
    "    return get_avg_fare_qry(input_tb=input_tb, index_col=index)\n",
    "\n",
    "\n",
    "def get_monthly_avg_fare_qry(input_tb: str) -> str:\n",
    "    \"\"\" Function to get monthly avg fare\"\"\"\n",
    "    index = \"tpep_pickup_datetime.strftime('%Y-%m') as pickup_month\"\n",
    "    return get_avg_fare_qry(input_tb=input_tb, index_col=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_read_qry = ddb_read_parquet(nytaxi_files)\n",
    "_agg_qry = get_daily_avg_fare_qry(input_tb='base')\n",
    "\n",
    "if 1==0:_read_qry = \"select * from ny_taxi\" # incase the https returns 403 \n",
    "\n",
    "daily_agg = f\"\"\" \n",
    "with base AS (\n",
    "    {_read_qry}\n",
    ")\n",
    "{_agg_qry}\n",
    "\"\"\"\n",
    "print(daily_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(daily_agg).limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_agg_qry = get_monthly_avg_fare_qry(input_tb='base')\n",
    "\n",
    "monthly_agg = f\"\"\" \n",
    "with base AS (\n",
    "    {_read_qry}\n",
    "),\n",
    "agg_tb AS (\n",
    "    {_agg_qry}\n",
    ")\n",
    "select \n",
    "    *\n",
    "from agg_tb\n",
    "where pickup_month between '2024-01' and '2024-09'\n",
    "order by pickup_month\n",
    "\"\"\"\n",
    "print(monthly_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(monthly_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling Statements\n",
    "* Describe: gets the schema of the output query\n",
    "* Explain: Query plan without executing it\n",
    "* Explain Analyze: Query plan with execution time.\n",
    "\n",
    "* reference: https://duckdb.org/docs/sql/statements/profiling.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkthe schema of the output query\n",
    "print(\"Check the schema using the DESCRIBE statement\")\n",
    "_desc_qry = f\"DESCRIBE {_read_qry}\"\n",
    "con.sql(_desc_qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # helper function.\n",
    "# ddb_describe = lambda _qry:con.sql(f\"DESCRIBE {_qry}\")\n",
    "# ddb_sql = lambda _qry:con.sql(f\"{_qry}\")\n",
    "# ddb_get_reltb = lambda _tbname: con.sql(f\"from {_tbname}\") \n",
    "# ddb_explain = lambda _qry:print(con.sql(f\"EXPLAIN {_qry}\").fetchall()[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Friendlier SQL\n",
    "* Great blog posts by Alex Monahan\n",
    "    * https://duckdb.org/2022/05/04/friendlier-sql.html\n",
    "    * https://duckdb.org/2023/08/23/even-friendlier-sql.html\n",
    "* highlights\n",
    "    * select * Exclude \n",
    "    * select * Replace \n",
    "    * COLUMNS \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natively reads Pandas DF\n",
    "* No more serialization, de-serialization and socket data transfer! \n",
    "* duckdb can read pandas df without the need for serialization and de-serialization. We can execute SQL commands on pdf without significant overhead!!! \n",
    "* same for arrow and polars\n",
    "* certain joins such as value between A and B is more efficient in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris dataset from seaborn\n",
    "iris_df = sns.load_dataset('iris')\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(iris_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each species get the max value of each column\n",
    "# we use the COLUMNS(* EXCLUDE ..) expression to select all columns excpet species\n",
    "_qry = \"\"\" \n",
    "select\n",
    "    species,\n",
    "    MAX(COLUMNS(* EXCLUDE species))\n",
    "    -- max(sepal_length), max(sepal_width), max(petal_length), max(petal_width)\n",
    "from iris_df\n",
    "group by all\n",
    "order by species\n",
    "\"\"\"\n",
    "con.sql(_qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python API\n",
    "* lazy execution\n",
    "* can pipe multiple \n",
    "* Reference:\n",
    "    * https://duckdb.org/docs/api/python/overview\n",
    "    * Relational API: https://duckdb.org/docs/api/python/relational_api\n",
    "    * Functional API: https://duckdb.org/docs/api/python/function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the relational table\n",
    "ny_taxi_tb = con.sql(\"select * from ny_taxi\")\n",
    "\n",
    "# duckdb also support from table select cols in addition  to the traditional select cols from table\n",
    "if 1==0: ny_taxi_tb = con.sql(\"from ny_taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ny_taxi_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can conver duckdb table to arrow/pandas/polars/etc.\n",
    "# reference: https://duckdb.org/docs/api/python/overview\n",
    "# to pandas\n",
    "display(ny_taxi_tb.limit(10).df())\n",
    "\n",
    "# to arrow\n",
    "if 1==0: ny_taxi_tb.limit(10).arrow()\n",
    "\n",
    "# to polars\n",
    "if 1==0: ny_taxi_tb.limit(10).pl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support chaining operations\n",
    "(\n",
    "    ny_taxi_tb\n",
    "    # variable.fun1().fun2()... as opposed to func2(fun1(vairable))\n",
    "    .aggregate(\"tpep_pickup_datetime.min(),tpep_pickup_datetime.max()\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_st_time = datetime.now()\n",
    "_=plt.figure(figsize=(10,7))\n",
    "_=\\\n",
    "(\n",
    "    ny_taxi_tb\n",
    "    .filter(\"tpep_pickup_datetime between '2024-01-01' and '2024-9-30'\")\n",
    "    # daily avg_fare\n",
    "    .aggregate(\"\"\" \n",
    "               tpep_pickup_datetime::date as pickup_date,\n",
    "               total_amount.sum().round()::int as tot_amt,\n",
    "               sum(trip_distance).round()::int as tot_dist,\n",
    "               (tot_amt/tot_dist).round(2) as avg_fare,\n",
    "               count(1) as counts,\n",
    "               \"\"\"\n",
    "            )\n",
    "    .order('pickup_date')\n",
    "    .df()\n",
    "    .pipe(sns.lineplot,x='pickup_date',y='avg_fare')\n",
    ")\n",
    "\n",
    "_=plt.title(\"Avg Fare by Pickup Date\")\n",
    "_=get_elapsed_time(_st_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not the best way to measure max memory used by duckdb. \n",
    "# to highlight the memory usage between duckdb and pandas \n",
    "memory_used()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert output to Pandas or Arrow\n",
    "# ~30M rows. ~4GB in Pandas\n",
    "ny_taxi_df = ny_taxi_tb.df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_taxi_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_used()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pandas\n",
    "_st_time = datetime.now()\n",
    "_=plt.figure(figsize=(10,7))\n",
    "_=\\\n",
    "(\n",
    "    ny_taxi_df\n",
    "    .query(\"tpep_pickup_datetime >= '2024-01-01' and tpep_pickup_datetime <='2024-9-30'\")\n",
    "    .assign(pickup_date=lambda x:x['tpep_pickup_datetime'].dt.date)\n",
    "    .groupby('pickup_date')\n",
    "    .agg(\n",
    "        tot_amt=('total_amount','sum'),\n",
    "        tot_dist = ('trip_distance','sum'),\n",
    "    )\n",
    "    .assign(avg_fare=lambda df:df['tot_amt']/df['tot_dist'])\n",
    "    .sort_index()\n",
    "    .reset_index()\n",
    "    .pipe(sns.lineplot,x='pickup_date',y='avg_fare')\n",
    ")\n",
    "_=plt.title(\"Avg Fare by Pickup Date\")\n",
    "_=get_elapsed_time(_st_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_used()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the best of Python + SQL\n",
    "* Better code organization using function and classes, for loops, etc.,\n",
    "* wider python eco system for plotting,ML, etc., \n",
    "* Awesome for using db storage for saving all tables related to ML projects - raw data, features, etc., Makes it easy to update and share. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling outside the Warehouse using AWS Lambda\n",
    "* Lambda function with required tagging. \n",
    "* invoke lambda function -> request_id\n",
    "* get CloudWatch Logs\n",
    "* track status of each request_id : started, success/error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "log_client = boto3.client('logs')\n",
    "\n",
    "# create s3fs object\n",
    "creds = boto3.Session().get_credentials().get_frozen_credentials()\n",
    "_s3fs = s3fs.S3FileSystem(key=creds.access_key,secret=creds.secret_key,skip_instance_cache=True)\n",
    "\n",
    "def get_s3_file_info(files: list):\n",
    "    \"\"\" Function to get file info of s3 files\"\"\"\n",
    "    _files = files if isinstance(files,list) else [files]\n",
    "    return pd.DataFrame([_s3fs.info(file) for file in _files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregate_qry(input_path: str|list,output_path: str=None) -> str:\n",
    "    \"\"\" \n",
    "    Function to get daily aggregate\n",
    "    input: https path\n",
    "    \"\"\"\n",
    "    # if input path is str (signle file) convert to list\n",
    "    input_path = input_path if isinstance(input_path,list) else [input_path]\n",
    "    \n",
    "    _agg_qry = f\"\"\" \n",
    "        select \n",
    "            tpep_pickup_datetime::date as pickup_date,\n",
    "            total_amount.sum().round()::int as tot_amt,\n",
    "            sum(trip_distance).round()::int as tot_dist,\n",
    "            (tot_amt/tot_dist).round(2) as avg_fare,\n",
    "            count(*) as counts\n",
    "        from read_parquet({input_path})\n",
    "        group by all\n",
    "    \"\"\"\n",
    "    \n",
    "    if output_path:\n",
    "        _final_qry = f\"\"\" \n",
    "        COPY (\n",
    "            {_agg_qry}\n",
    "        ) \n",
    "        TO '{output_path}' (FORMAT PARQUET,OVERWRITE true)\n",
    "        \"\"\"\n",
    "    else:\n",
    "        _final_qry = _agg_qry\n",
    "    \n",
    "    return _final_qry\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nytaxi_tasks(months:list,s3_output_base) -> dict:\n",
    "    \"\"\" \n",
    "    Function to generate the tasks for each month\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_output_path(month: str) -> str:\n",
    "        return f\"{s3_output_base.rstrip('/')}/agg_{month}.parquet\"\n",
    "\n",
    "    \n",
    "    def get_qry(month: str) ->str:\n",
    "        \"\"\" \n",
    "        get the aggregate query for the specified input/output paths\n",
    "        \"\"\"\n",
    "        _input_file = get_nytaxi_data_path(month)\n",
    "        _output_file = get_output_path(month)\n",
    "        return get_aggregate_qry(\n",
    "            input_path=_input_file,\n",
    "            output_path=_output_file\n",
    "        )\n",
    "    \n",
    "    def create_task(month: str) -> dict:\n",
    "        \"\"\" create (single) task for month\n",
    "        \"\"\"    \n",
    "        return {\n",
    "            'qry':get_qry(month),\n",
    "            'month':month,\n",
    "            'output_path':get_output_path(month)\n",
    "        }\n",
    "    \n",
    "    def create_tasks() -> dict:\n",
    "        \"\"\" create multiple tasks for input list (months)\n",
    "        \"\"\"\n",
    "        print(f\"Generating {len(months)} tasks\")\n",
    "        return {month:create_task(month) for month in months}\n",
    "\n",
    "    return create_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# months = get_months(\"2020-01\",\"2024-09\")\n",
    "months = get_months('2024-01','2024-09')\n",
    "output_base = \"s3://adarshnamala/pydata_demo/ny_taxi/lambda/\"\n",
    "tasks = generate_nytaxi_tasks(months,s3_output_base=output_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks[months[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tasks[months[0]]['qry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now(UTC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_tasks = DuckdbLambda_Tasks(\n",
    "    tasks=tasks,\n",
    "    lambda_client=lambda_client,\n",
    "    log_client=log_client\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the tasks\n",
    "lambda_tasks.event_invoke_tasks(qry_key='qry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_tasks.wait_until_tasks_complete(total_wait_time_secs=60,interval_check_time_sec=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the last modified date of the output files\n",
    "_files = [task['output_path'] for task in tasks.values()]\n",
    "(\n",
    "    get_s3_files_info(_files)\n",
    "    ['LastModified'].agg(['min','max','size'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read daily_agg parquest from S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_outfiles = [_['output_path'] for _ in tasks.values()]\n",
    "_read_qry = ddb_read_parquet(_outfiles)\n",
    "\n",
    "con.sql(_read_qry).limit(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run for 60 months**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# months = get_months(\"2020-01\",\"2024-09\")\n",
    "months = get_months('2019-10','2024-09')\n",
    "output_base = \"s3://adarshnamala/pydata_demo/ny_taxi/lambda/\"\n",
    "tasks = generate_nytaxi_tasks(months,s3_output_base=output_base)\n",
    "\n",
    "lambda_tasks = DuckdbLambda_Tasks(tasks=tasks,\n",
    "                      lambda_client=lambda_client,\n",
    "                      log_client=log_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now(UTC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_tasks.event_invoke_tasks(qry_key='qry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_tasks.wait_until_tasks_complete(total_wait_time_secs=60,interval_check_time_sec=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the last modified date of the output files\n",
    "_files = [task['output_path'] for task in tasks.values()]\n",
    "(\n",
    "    get_s3_files_info(_files)\n",
    "    ['LastModified'].agg(['min','max','size'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling using Coiled.io\n",
    "* challenges in managing cluster:\n",
    "    * start/stop\n",
    "    * install packages\n",
    "    * copy code\n",
    "    \n",
    "* notebooks: https://docs.coiled.io/user_guide/notebooks.html\n",
    "* serverless: https://docs.coiled.io/user_guide/functions.html\n",
    "* price: https://www.coiled.io/pricing\n",
    "* build vs buy: https://www.coiled.io/build-vs-buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coiled.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline settings for coiled Function params\n",
    "ip_address= '170.85.72.183'\n",
    "tags ={'Application ID':'RSH'}\n",
    "coiled_function_params = dict(region='us-east-1',\n",
    "                 arm=True,\n",
    "                 idle_timeout='30 seconds',\n",
    "                 spot_policy='spot',\n",
    "                 memory='8 GB',\n",
    "                 threads_per_worker=1,\n",
    "                 name='pydata_demo',\n",
    "                 allow_ingress_from = ip_address,\n",
    "                 tags=tags)\n",
    "\n",
    "\n",
    "def setup_ddb_con():\n",
    "    \"\"\" \n",
    "    function to create the duckdb connection with AWS credentials\n",
    "    \"\"\"\n",
    "    con = duckdb.connect(\":memory:\")\n",
    "    _aws_creds = \"\"\" \n",
    "    INSTALL AWS; LOAD AWS;\n",
    "    CREATE SECRET secret2 (TYPE S3, PROVIDER CREDENTIAL_CHAIN);\n",
    "    \"\"\"\n",
    "    con.sql(_aws_creds)\n",
    "    return con\n",
    "    \n",
    "\n",
    "@coiled.function(**coiled_function_params)\n",
    "def run_coiled_tasks(input: dict):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    \n",
    "    _st = datetime.now()\n",
    "    qry = input['qry']\n",
    "    name = input['month']\n",
    "    print(f\"Running for task:{name}\")\n",
    "    try:\n",
    "        # initiate duckdb connection\n",
    "        con = setup_ddb_con()\n",
    "        _ =con.sql(qry)\n",
    "        print(f\"completed Task: {name} in {(datetime.now() - _st).seconds:.2f} seconds\")\n",
    "        val = True\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Task: {name}\")\n",
    "        print(e)\n",
    "        val = False\n",
    "        \n",
    "    \n",
    "    return val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = get_months('2024-01','2024-09')\n",
    "output_base = \"s3://adarshnamala/pydata_demo/ny_taxi/coiled/\"\n",
    "tasks = generate_nytaxi_tasks(months,s3_output_base=output_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks[months[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1==1:\n",
    "    run_coiled_tasks.cluster.adapt(minimum=len(tasks),maximum=len(tasks))\n",
    "    results = run_coiled_tasks.map(list(tasks.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_status = list(results)\n",
    "print(f\"Completed {sum(task_status)}/{len(task_status)} tasks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coiled with Jupyter notbeooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.rand(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
